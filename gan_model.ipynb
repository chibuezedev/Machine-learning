# !pip install faker
# !pip install tensorflow

import numpy as np
import pandas as pd
from faker import Faker
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Initialize Faker for synthetic data generation
fake = Faker()


# Generate a small dataset to train the GAN
def create_real_data(num_samples):
    records = []
    for _ in range(num_samples):
        # Generate random transaction data
        amount = round(np.random.uniform(10.0, 2000.0), 2)
        location = np.random.choice(['New York', 'Los Angeles', 'San Francisco', 'Houston', 'Chicago'])
        records.append([amount, location])
    return np.array(records)


# Create a small dataset of real transactions
real_data = create_real_data(1000)  # Create 1000 samples for training


# Preprocess the data
def preprocess_data(data):
    # Normalize the numerical data (amount)
    amounts = data[:, 0].astype(np.float32) # Convert amounts to float32
    locations = data[:, 1]

    # Encode categorical location data (one-hot encoding)
    location_encoded = pd.get_dummies(locations)

    return np.hstack((amounts.reshape(-1, 1), location_encoded.values.astype(np.float32))) # Convert one-hot to float32


processed_real_data = preprocess_data(real_data)


# Create the generator model
def build_generator():
    model = keras.Sequential()
    model.add(layers.Dense(128, activation='relu', input_dim=10))  # Input dimension should match the noise vector
    model.add(layers.Dense(6, activation='linear'))  # Output will have 1 amount and 5 encoded locations
    return model


# Create the discriminator model
def build_discriminator():
    model = keras.Sequential()
    model.add(layers.Dense(128, activation='relu', input_shape=(6,)))  # Changed input_shape to (6,)
    model.add(layers.Dense(1, activation='sigmoid'))  # Output is a probability
    return model


# Build and compile the models
generator = build_generator()
discriminator = build_discriminator()

discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])


# Create the GAN model
def build_gan(generator, discriminator):
    discriminator.trainable = False  # Freeze the discriminator when training the GAN
    model = keras.Sequential([generator, discriminator])
    model.compile(loss='binary_crossentropy', optimizer='adam')
    return model


gan = build_gan(generator, discriminator)


# Training the GAN
def train_gan(epochs, batch_size):
    for epoch in range(epochs):
        # Train the discriminator
        idx = np.random.randint(0, processed_real_data.shape[0], batch_size)
        real_samples = processed_real_data[idx]

        noise = np.random.normal(0, 1, (batch_size, 10))  # Random noise
        fake_samples = generator.predict(noise)

        # Labels for real and fake samples
        real_labels = np.ones((batch_size, 1))
        fake_labels = np.zeros((batch_size, 1))

        # Train the discriminator
        d_loss_real = discriminator.train_on_batch(real_samples, real_labels)
        d_loss_fake = discriminator.train_on_batch(fake_samples, fake_labels)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train the generator
        noise = np.random.normal(0, 1, (batch_size, 10))  # Random noise
        g_loss = gan.train_on_batch(noise, real_labels)  # We want to fool the discriminator

        # Print the progress
        if (epoch + 1) % 100 == 0:
            print(f"{epoch + 1}/{epochs} [D loss: {d_loss[0]:.4f}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]")


# Train the GAN for 5000 epochs with a batch size of 32
train_gan(epochs=5000, batch_size=32) # reduce the epoch to 50 and test


# Generate synthetic data using the trained generator
def generate_synthetic_data(num_samples):
    noise = np.random.normal(0, 1, (num_samples, 10))
    generated_samples = generator.predict(noise)
    generated_samples = np.array(generated_samples)

    # Post-process the generated samples
    amounts = generated_samples[:, 0]
    locations_encoded = generated_samples[:, 1:]  # One-hot encoded locations
    locations = np.argmax(locations_encoded, axis=1)  # Convert back to indices

    # Map indices to original location names
    location_names = ['New York', 'Los Angeles', 'San Francisco', 'Houston', 'Chicago']
    location_output = [location_names[i] for i in locations]

    return np.column_stack((amounts, location_output))


# Generate 100 synthetic transaction samples
synthetic_data = generate_synthetic_data(100)

# Convert synthetic data to DataFrame for easy viewing
synthetic_df = pd.DataFrame(synthetic_data, columns=['Amount', 'Location'])
print(synthetic_df.head())

# Optionally, save to CSV
synthetic_df.to_csv('synthetic_transaction_data_gan.csv', index=False)
